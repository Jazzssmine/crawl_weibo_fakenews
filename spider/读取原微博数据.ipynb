{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**原微博**\n",
    "* 内容--done\n",
    "* 发布时间--done\n",
    "* 点赞/转发数--done\n",
    "* 评论数/评论内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import urllib\n",
    "import re\n",
    "import unidecode\n",
    "import time\n",
    "import requests\n",
    "from requests import Request, Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari / 537.36'\n",
    "}\n",
    "cookie = {\n",
    "    'ALF': '1569558716', \n",
    "    'SCF': 'Am7Jt7ySAY5qGVf-9s3G4Of_Q9InAuL_1ZZY7JWBEh1EiWnBRLXtcSh1nR4wbrIyDDff11BLAnQju0tu2lJWWvw.',\n",
    "    'SUBP': '0033WrSXqPxfM725Ws9jqgMF55529P9D9WWJcHiLLQYmQJX._sM6GNLg5JpX5KzhUgL.FoeX1h-7ehMcehM2dJLoIN-LxKMLB.zL1hnLxKBLB.2L12-LxK-L12qLB-qLxKBLB.zLBK-LxKqL1KBLBo.LxK-L1K5L12BLxKBLB.2L1hqLxK-L12qL1hnLxKnLBKqL1h2LxK-L1K5L12-LxKqL1hzL1KBLxKBLBonL1h5LxK-LBo.LBoBt', \n",
    "    '_T_WM': '61257250789',\n",
    "    'SUB': '_2A25wYnR2DeRhGeVK41cR8CnKyzuIHXVTrRw-rDV6PUJbkdBeLVeskW1NTCdnposyBx280rTF2p9x6qqSkZWoX8eQ', \n",
    "    'SUHB': '0YhRmMMZUqsJLt' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_path = []\n",
    "for i in range(0, 50):\n",
    "    crawl_path.append('crawl_data/text_data' + str(i) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "deleted\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "deleted\n",
      "13\n",
      "14\n",
      "deleted\n",
      "15\n",
      "16\n",
      "17\n",
      "deleted\n",
      "18\n",
      "19\n",
      "deleted\n",
      "deleted\n",
      "deleted\n",
      "deleted\n",
      "deleted\n",
      "deleted\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "deleted\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "deleted\n",
      "19\n",
      "deleted\n",
      "deleted\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "deleted\n",
      "17\n",
      "deleted\n",
      "18\n",
      "19\n",
      "deleted\n",
      "deleted\n",
      "0\n",
      "1\n",
      "2\n",
      "deleted\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "deleted\n",
      "13\n",
      "14\n",
      "15\n",
      "deleted\n",
      "16\n",
      "17\n",
      "deleted\n",
      "18\n",
      "deleted\n",
      "19\n",
      "deleted\n",
      "deleted\n",
      "deleted\n",
      "deleted\n",
      "deleted\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "deleted\n",
      "8\n",
      "9\n",
      "10\n",
      "deleted\n",
      "11\n",
      "deleted\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "deleted\n",
      "deleted\n",
      "deleted\n"
     ]
    }
   ],
   "source": [
    "for j in range(0, 5):\n",
    "    crawl_data = pd.read_csv(crawl_path[j])\n",
    "    weibo_content = []\n",
    "    repost_num = []\n",
    "    thumbs_num = []\n",
    "    comment_num = []\n",
    "    comments_all = []\n",
    "    for i in range(0, 20):\n",
    "        print(i)\n",
    "        url = crawl_data['link_to_post'][i]\n",
    "        if url != 'nn':\n",
    "            r = requests.get(url, cookies=cookie)\n",
    "            content = r.text\n",
    "            soup = BeautifulSoup(content, 'lxml')\n",
    "        \n",
    "            ##微博内容\n",
    "            for item in soup.select('div[class=\"c\"] div span[class=\"ctt\"]' ):\n",
    "                weibo_content.append(item.get_text())\n",
    "                #print(weibo_content)\n",
    "            \n",
    "            ##转发\n",
    "            for item in soup.findAll(\"a\",{\"href\":re.compile(\"\\/repost\\/(.+)\")}):\n",
    "                repost = re.findall(r'[0-9]+', str(item.get_text()))\n",
    "                if len(repost) == 0:\n",
    "                    repost_num.append('0')\n",
    "                else:\n",
    "                    repost_num.append(repost[0])\n",
    "                #print(repost_num)\n",
    "                \n",
    "            ##赞\n",
    "            for item in soup.findAll(\"a\",{\"href\":re.compile(\"\\/attitude\\/(.+)\\?\\#\")}):\n",
    "                thumbs = re.findall(r'[0-9]+', str(item.get_text()))\n",
    "                thumbs_num.append(thumbs[0])\n",
    "            \n",
    "            ##评论数\n",
    "            for item in soup.findAll('span', {'class': 'pms'}):\n",
    "                comment = re.findall(r'[0-9]+', str(item.get_text()))\n",
    "                if len(comment) == 0:\n",
    "                    comment_num.append('0')\n",
    "                else:\n",
    "                    comment_num.append(comment[0])\n",
    "        \n",
    "            if len(soup.select('div[class=\"c\"] div span[class=\"ctt\"]' )) == 0:\n",
    "                weibo_content.append('no access')\n",
    "                repost_num.append('no access')\n",
    "                thumbs_num.append('no access')\n",
    "                comment_num.append('no access')\n",
    "                \n",
    "        else:\n",
    "            print('deleted')\n",
    "            weibo_content.append('deleted')\n",
    "            repost_num.append('deleted')\n",
    "            thumbs_num.append('deleted')\n",
    "            comment_num.append('deleted')\n",
    "    \n",
    "    ##评论\n",
    "    for i in range(0, 20):\n",
    "        url = crawl_data['link_to_post'][i]\n",
    "        comments = []\n",
    "        if url != 'nn':\n",
    "            r = requests.get(url, cookies=cookie)\n",
    "            content = r.text\n",
    "            soup = BeautifulSoup(content, 'lxml')\n",
    "            #print(url)        \n",
    "            ## 页数\n",
    "            if len(soup.select('div[class=\"pa\"] form div input')) == 0:\n",
    "                comments.append('no comment accessible')\n",
    "            else:\n",
    "                page_une = soup.select('div[class=\"pa\"] form div input')[0]\n",
    "                page_num = re.findall(r\"\\d+\",str(page_une))\n",
    "        \n",
    "                for p in range(1, min(4, int(page_num[0]) + 1)):\n",
    "                    comment_web = url + '?page=' + str(p)\n",
    "                    comment_r = requests.get(comment_web, cookies=cookie)\n",
    "                    comment = comment_r.text\n",
    "                    comment_soup = BeautifulSoup(comment, 'lxml')\n",
    "                    for item in comment_soup.select('div[class=\"c\"] span[class=\"ctt\"]'):\n",
    "                        cc = item.get_text()\n",
    "                        comments.append(cc)\n",
    "\n",
    "        else:\n",
    "            print(\"deleted\")\n",
    "            comments.append('deleted')\n",
    "    \n",
    "        comments_all.append(comments)\n",
    "    \n",
    "    \n",
    "    \n",
    "    crawl_data['comments'] = comments_all\n",
    "    crawl_data['weibo_content'] = weibo_content\n",
    "    crawl_data['repost_num'] = repost_num\n",
    "    crawl_data['thumbs_up'] = thumbs_num\n",
    "    crawl_data['comment_num'] = comment_num\n",
    "    crawl_data.to_csv('crawl_data/crawl_' + str(j) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://weibo.cn/comment/I1tm7zm98\n",
      "https://weibo.cn/comment/I1yLyzrSI\n",
      "https://weibo.cn/comment/I1rg3Acxm\n",
      "https://weibo.cn/comment/I1rjuCZCd\n",
      "https://weibo.cn/comment/I1sxOmaRI\n",
      "https://weibo.cn/comment/I1yiyf91T\n",
      "https://weibo.cn/comment/I1rEz341I\n",
      "https://weibo.cn/comment/I1r4LAqjo\n",
      "https://weibo.cn/comment/I1sKW9ult\n",
      "https://weibo.cn/comment/I1pJLzBkT\n",
      "https://weibo.cn/comment/I1yTU02qd\n",
      "https://weibo.cn/comment/I1xzHCAI0\n",
      "https://weibo.cn/comment/I1przmR59\n",
      "https://weibo.cn/comment/I1qU4ur1S\n",
      "https://weibo.cn/comment/I1sV3rl76\n",
      "https://weibo.cn/comment/I1Crq76cs\n",
      "deleted\n",
      "deleted\n",
      "https://weibo.cn/comment/I1BeShFSq\n",
      "https://weibo.cn/comment/I1revf6wd\n"
     ]
    }
   ],
   "source": [
    "'''comments_all = []\n",
    "for i in range(0, 20):\n",
    "    url = crawl_data['link_to_post'][i]\n",
    "    comments = []\n",
    "    if url != 'nn':\n",
    "        r = requests.get(url, cookies=cookie)\n",
    "        content = r.text\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        print(url)        \n",
    "        ## 页数\n",
    "        if len(soup.select('div[class=\"pa\"] form div input')) == 0:\n",
    "            comments.append('no comment accessible')\n",
    "        else:\n",
    "            page_une = soup.select('div[class=\"pa\"] form div input')[0]\n",
    "            page_num = re.findall(r\"\\d+\",str(page_une))\n",
    "        \n",
    "            for p in range(1, min(4, int(page_num[0]) + 1)):\n",
    "                comment_web = url + '?page=' + str(p)\n",
    "                comment_r = requests.get(comment_web, cookies=cookie)\n",
    "                comment = comment_r.text\n",
    "                comment_soup = BeautifulSoup(comment, 'lxml')\n",
    "                for item in comment_soup.select('div[class=\"c\"] span[class=\"ctt\"]'):\n",
    "                    cc = item.get_text()\n",
    "                    comments.append(cc)\n",
    "\n",
    "    else:\n",
    "        print(\"deleted\")\n",
    "        comments.append('deleted')\n",
    "    \n",
    "    comments_all.append(comments)\n",
    "    crawl_data['comments'] = comments_all'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     https://weibo.cn/comment/I1stFzakR\n",
       "1     https://weibo.cn/comment/I1JwChWGU\n",
       "2     https://weibo.cn/comment/I1J5GswQN\n",
       "3     https://weibo.cn/comment/I1CichXQW\n",
       "4     https://weibo.cn/comment/I1Qet6W2M\n",
       "5                                     nn\n",
       "6     https://weibo.cn/comment/I1Thx0hw1\n",
       "7     https://weibo.cn/comment/HvX3iyVca\n",
       "8     https://weibo.cn/comment/I1SUcwz9E\n",
       "9     https://weibo.cn/comment/Hy4iEAtoK\n",
       "10    https://weibo.cn/comment/I1GU3Acbz\n",
       "11    https://weibo.cn/comment/I1PL08JZs\n",
       "12    https://weibo.cn/comment/I32Uf8fGM\n",
       "13    https://weibo.cn/comment/I2ZZgoEL8\n",
       "14    https://weibo.cn/comment/I0Dn8fQhI\n",
       "15    https://weibo.cn/comment/I0DnqxRyW\n",
       "16    https://weibo.cn/comment/I0FeazVdq\n",
       "17    https://weibo.cn/comment/I2Yzj6Nk3\n",
       "18                                    nn\n",
       "19    https://weibo.cn/comment/I37cvDxnq\n",
       "Name: link_to_post, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_data['link_to_post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_data = pd.read_csv(crawl_path[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
